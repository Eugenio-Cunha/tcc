\chapter{Fundamentação Teórica}\label{fund_teo}

Este capítulo irá abordar alguns conceitos que serão necessários durante o desenvolvimento
deste trabalho para que o projeto tenha fundamentos concretos.

\section{Processamento de linguagem natural}

O desenvolvimento de modelos computacionais para a realização de tarefas que dependem de informações expressas em uma língua natural também conhecido como processamento de linguagem natural, cresce desde o início da década de 1990, segundo ~\cite{vieira2010processamento} "o crescimento da internet e a profusão de textos disponíveis direcionaram os esforços do PLN para o tratamento de textos mais do que para o discurso falado". Ainda segundo o autor neste mesmo período iniciou as pesquisas sobre conjuntos de textos sobre um domínio de conhecimento onde cada uma das suas palavras foram identificadas segundo sua função sintática.

O processamento de linguagem natural segundo o trabalho de ~\cite{teixeira2011analise} pode envolver diversas etapas: \textit{tokenizer} ou divisão do texto em termos mais simples, \textit{phrase chunking} ou análise sintática e \textit{part-of-speech tagging} ou identificação da classe gramatical das palavras. Existem também outras etapas mais específicas, como a identificação de entidades (datas, nomes, número, etc), entretanto algumas destas etapas são essenciais para o processamento correto do texto, como o \textit{tokenizer}.    

\section{\textit{Bag of words}}

Segundo ~\cite{alexandra_alves:2010} o BOW (\textit{Bag of Words}) é o modelo mais utilizado em aplicações de classificação de texto. Com baixo custo em termos de processamento este modelo transforma a cadeia de caracteres de um documento num conjunto de palavras, registrando além da presença de uma palavra, a sua frequência.

Entretanto ainda segundo o autor, propriedades básicas do texto, como a ordem em que as palavras ocorrem e a pontuação, são ignoradas, além da incapacidade em capturar a semântica do texto, isto é, há palavras com significados distintos que apesar de serem exatamente iguais têm significados diferentes, dependendo do contexto em que são utilizadas.

Obviamente, termos que aparecem em todos os documentos são denominados \textit{stop words} e não serão analisados, geralmente são os pronomes, artigos e as preposições. Estes termos não são úteis, visto que têm uma semântica fraca e somente desempenham um papel funcional no texto. Para melhorar os métodos de processamento normalmente são removidas, em vários casos a remoção das \textit{stop words} não traz consequências graves segundo o autor.

Na Tabela ~\ref{tab:bow}, \textbf{w}\textsubscript{i} representa uma palavra, \textbf{d}\textsubscript{j} representa um documento e \textbf{p}\textsubscript{ij} o peso atribuído a cada palavra no documento.

\begin{longtable}{|c|c|c|c|c|c|}
    \caption{Modelo \textit{Bag of Words} é maneira mais comum de representar coleções de documentos no qual cada documento é representado por um vetor e cada palavra da coleção representa uma dimensão do vetor.}
    \label{tab:bow}
    \endfirsthead
    \multicolumn{6}{l}%
    {Tabela \thetable{} (continuação)}
    \endhead
    \multicolumn{6}{l}%
    {Continua na próxima página}\\
    \endfoot
    \endlastfoot
    \hline  & \textbf{w}\textsubscript{1} & \textbf{w}\textsubscript{2} & \textbf{w}\textsubscript{3} & \textbf{w}\textsubscript{...} & \textbf{w}\textsubscript{n} \\
    \hline \textbf{d}\textsubscript{1} & p\textsubscript{11} & p\textsubscript{12} & p\textsubscript{13} & p\textsubscript{...} & p\textsubscript{1n} \\
    \hline \textbf{d}\textsubscript{2} & p\textsubscript{21} & p\textsubscript{22} & p\textsubscript{23} & p\textsubscript{...} & p\textsubscript{2n} \\
    \hline \textbf{d}\textsubscript{3} & p\textsubscript{31} & p\textsubscript{32} & p\textsubscript{33} & p\textsubscript{...} & p\textsubscript{3n} \\
    \hline \textbf{d}\textsubscript{4} & p\textsubscript{41} & p\textsubscript{42} & p\textsubscript{43} & p\textsubscript{...} & p\textsubscript{4n} \\
    \hline \textbf{d}\textsubscript{n} & p\textsubscript{n1} & p\textsubscript{n2} & p\textsubscript{n3} & p\textsubscript{...} & p\textsubscript{nn} \\
    \hline
\end{longtable}

Ainda segundo ~\cite{alexandra_alves:2010} existem várias medidas para calcular os valores dos pesos de p\textsubscript{ij}. Essas medidas podem ser classificadas em dois tipos distintos: baseadas em frequências e binárias. Os pesos baseados em frequência visam contabilizar o número de ocorrências de um dado termo num determinado documento, servindo como base para diversas medidas estatísticas e os pesos binários indicam a ocorrência ou não de um dado termo num determinado documento.

\section{Aprendizado de Máquina supervisionado}

De acordo com ~\cite{de2005anotaccao} no decorrer da última década, o Aprendizado de Máquina tem atestado ser uma ferramenta eficiente para realizar tarefas linguísticas, que de outro maneira seria impossível devido à enorme quantidade de mão-de-obra e tempo necessário. 

Segundo o trabalho de ~\cite{batista2003pre}, na área de Aprendizado de Máquina foram propostos diversos paradigmas aptos a aprender a partir de um conjunto de exemplos. Uma premissa básica para todos os paradigmas de Aprendizado de Máquina supervisionado é que o conceito a ser induzido deve ser referente ao caso observado, ou seja, cada exemplo deve estar denominado com a classe a qual pertence. 

Entretanto o autor cita que ``Se todos os casos são memorizados, o classificador pode se tornar lento e difícil de manusear. O ideal é reter casos prototípicos que juntos resumem toda uma informação importante.''.

\section{Classificador \textit{Adaboost}}

\textit{Boosting} é um conjunto de métodos e procedimentos de Aprendizado de Máquina que mescla vários classificadores fracos para aperfeiçoar a \textit{acurácia} geral. O algoritmo atualiza os pesos dos exemplos a cada iteração e cria um classificador adicional. Os classificadores são combinados por um esquema simples de votação. O algoritmo mais famoso baseado em Boosting é o Adaboost (Algoritmo Adaptive Boosting) que atualiza os pesos dos exemplos em que os classificadores anteriores cometeram erros, focando o classificador adicional nos exemplos mais difíceis ~\cite {duarte2009algoritmo}.

Merjildo et al. (2013) mostra as propriedades que simplificam o uso do classificador \textit{AdaBoost}. O autor cita em seu trabalho que os parâmetros empregados para analisar dados de grandes proporções e as margens entre as classes podem ser mais precisas do que em outros métodos e o custo computacional é baixo, dado que corresponde a um programa de complexidade linear e evita o uso de componentes computacionais pesados.

Por fim, este classificador escolhido para essa pesquisa, tem como objetivo reconhecer padrões complexos a partir de combinações de características simples baseadas em formas que podem estar presentes em textos de redação do tipo dissertativo-argumentativo.

\section{Métricas utilizadas}

Avaliar o desempenho de um modelo é um dos estágios principais deste estudo, a avaliação se baseia nas predições que são produzidas pelo modelo induzido. 

Na comparação dos resultados experimentais serão utilizadas sete métricas de avaliações sobre modelo supervisionado \textit{AdaBoost}, que são:

\textbf{Matriz de confusão} - A matriz de confusão de uma hipótese H oferece uma medida efetiva do modelo de classificação, ao mostrar o número de classificações corretas versus as classificações preditas para cada classe, sobre um conjunto de exemplos verdadeiros como descrito na Tabela ~\ref{tab:sample_matrix_confusion}.

\begin{table}[H]
\centering
\begin{tabular}{rc|c|c|}
\cline{3-4}
\multicolumn{2}{c|}{\multirow{2}{*}{}} & \multicolumn{2}{c|}{Valor verdadeiro} \\ \cline{3-4} 
\multicolumn{2}{c|}{} & positivos & negativos \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Valor previsto}} & positivos & VP & FP \\ \cline{2-4} 
\multicolumn{1}{|c|}{} & negativos & FN & VN \\ \hline
\end{tabular}
\caption{Matriz de confusão ou tabela de contingência}
\label{tab:sample_matrix_confusion}
\end{table}

\textbf{Acurácia} - porcentagem de amostras positivas e negativas classificadas corretamente sobre a soma de amostras positivas e negativas dada pela fórmula da equação ~\ref{eq:acuracia}:
\begin{equation} \label{eq:acuracia}
 acuracia = \frac{TP+TN}{VP+VN+FP+FN}
\end{equation}

\textbf{Sensitividade} - ou \textit{recall} é a porcentagem de amostras positivas classificadas corretamente sobre o total de amostras positivas dada pela fórmula da equação ~\ref{eq:recall}:
\begin{equation} \label{eq:recall}
  sensitividade = \frac{TP}{TP+FN} = \frac{TP}{Positivo}
\end{equation}

\textbf{Precisão} - ou \textit{precision} é a porcentagem de amostras positivas classificadas corretamente sobre o total de amostras classificadas como positivas dada pela fórmula da equação ~\ref{eq:precision}:
\begin{equation} \label{eq:precision}
    precisao = \frac{TP}{TP+FP}
\end{equation}

\textbf{Especificidade} - ou \textit{specificity} é a porcentagem de amostras negativas identificadas corretamente sobre o total de amostras negativas dada pela fórmula da equação ~\ref{eq:specificity}:
\begin{equation} \label{eq:specificity}
    especificidade = \frac{TN}{TN+FP}=\frac{TN}{Negativo}
\end{equation}

\textbf{\textit{F1}} - ou \textit{F-Measure} é uma média ponderada de precisão e sensitividade dada pela fórmula da equação ~\ref{eq:fscore}:
\begin{equation} \label{eq:fscore}
    f1=\frac{2*(precisao*sensitividade)}{precisao+sensitividade}
\end{equation}

\textbf{Curva ROC} - a Curva ROC é um gráfico da porcentagem de amostras corretamente classificadas como positivas dentre todas as positivas reais versus a porcentagem de amostras erroneamente classificadas como positivas dentre todas as negativas reais ou um \textit{trade-off} entre TPR e FPR, dadas pelas fórmulas da equações:

\begin{equation} \label{eq:tpr}
    TPR=\frac{TP}{TP+FN}
\end{equation}

\begin{equation} \label{eq:fpr}
    FPR=\frac{FP}{FP+TN}
\end{equation}

Obviamente, um modelo de classificação ideal teria TPR = 1 e FPR = 0 (taxa de acerto = 1 e taxa de erro = 0).

As principais métricas da literatura citadas neste estudo tem o objetivo de avaliar diretamente, de forma independente, o desempenho do classificador induzido.
